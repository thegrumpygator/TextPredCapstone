---
title: "Text Prediction Data Exploration"
author: "Brian L. Fuller"
date: "March 20, 2016"
output: 
  html_document: 
    keep_md: yes
---

# Asignment

The goal of this project is just to display that you've gotten used to working
with the data and that you are on track to create your prediction algorithm.
Please submit a report on R Pubs (http://rpubs.com/) that explains your
exploratory analysis and your goals for the eventual app and algorithm. This
document should be concise and explain only the major features of the data you
have identified and briefly summarize your plans for creating the prediction
algorithm and Shiny app in a way that would be understandable to a non-data
scientist manager. You should make use of tables and plots to illustrate
important summaries of the data set. The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app. 

Review criteria

* Does the link lead to an HTML page describing the exploratory analysis of
  the training data set? 
* Has the data scientist done basic summaries of the three files? 
* Word counts, line counts and basic data tables? 
* Has the data scientist made basic plots, such as histograms to illustrate 
  features of the data? 
* Was the report written in a brief, concise style, in a way that a
  non-data scientist manager could appreciate?

# Background

Text input prediction is becoming increasingly important in our modern world of
small handheld and wearable computing devices such as cellular phones, phablets,
tablets, smart watches, and  fitness wearables. These devices which do not have
physical keyboards present a challenge to the user when it comes to text entry.
It is imperative that these devices continue to improve in their ability to 
predict what the user will type and suggest increasingly accurate words and
phrases. The increasing computing power of these devices and internet access
should provide a basis to improve text input prediction. This preliminary data
analysis is the first step in my efforts to make such an improvement.

The data that will be used in this study comes from Swiftkey, located at the 
following URL:
[https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip](d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

The linked zip file contains three text files for training predictive text 
algorithms:

1. File containing many twitter posts
2. File containing many blog entries
3. File containing many news stories.

This set of data or samples from it will be used to determine commonly used 
words and phrases to build an algorithm to predict the most likely word that
a user would type next.

```{r Common Section, cache = TRUE, warning = FALSE, message = FALSE, echo = FALSE}
# set paths to data files
blogfile <- "Coursera-SwiftKey/final/en_US/en_US.blogs.txt"
newsfile <- "Coursera-SwiftKey/final/en_US/en_US.news.txt"
twitfile <- "Coursera-SwiftKey/final/en_US/en_US.twitter.txt"

# open connections to the three files
blogcon <- file(blogfile, open="rb")
newscon <- file(newsfile, open = "rb")
twitcon <- file(twitfile, open = "r")

# read the lines from the three files
blogLines <- readLines(blogcon, encoding = "UTF-8")
blogLines <- iconv(blogLines, "UTF-8", "ascii", sub = " ")

newsLines <- readLines(newscon, encoding = "UTF-8")
newsLines <- iconv(newsLines, "UTF-8", "ascii", sub = " ")

twitLines <- readLines(twitcon, encoding = "UTF-8")
twitLines <- iconv(twitLines, "UTF-8", "ascii", sub = " ")


# close the connectinos to the three files
close(blogcon)
close(newscon)
close(twitcon)

# subset the data from the three files
#subblog <- blogLines[1:10]

sampleSize <- 0.01
set.seed(11111)

subblog <- sample(blogLines, length(blogLines)*sampleSize)
subnews <- sample(newsLines, length(newsLines)*sampleSize)
subtwit <- sample(twitLines, length(twitLines)*sampleSize)

# combine the samples from the three files
subSet <- c(subnews, subblog, subtwit)

subSet2 <-paste0(subSet, collapse = " ")
#subSet <- subblog  ## temp while debugging

rm(blogcon)
rm(newscon)
rm(twitcon)
rm(blogfile)
rm(newsfile)
rm(twitfile)
#-------------------------------------------------------------------------------
```

# Data Characterization

Each row of the twitter file is a single tweet. Each row of the blog file is a 
single blog post. Each line of the news file is a single news story. The number
of entries is as follows:

Source        | Number
--------------|--------
Tweets        | `r as.numeric(summary(twitLines)[1])`
Blog Posts    | `r as.numeric(summary(blogLines)[1])`
News Stories  | `r as.numeric(summary(newsLines)[1])`

The total number of words in the full text is
```{r characterization, cache = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
library("quanteda")
totaltext <- paste0(twitLines, blogLines, newsLines, collapse = " ")
corpustotal <- corpus(totaltext)
#nsentence(corpustotal)
#nsentence(totaltext)
ntoken(corpustotal)
#ntoken(totaltext)
```

# Proprocess the data

Since the data comes from the internet and contains some non-word data such as 
symbles and data from unreadable encodings, several *cleaning* steps will be 
performed. Specifically, I will convert the words to all lower case, remove 
numbers, remove punctuation, and strip any leftover extra white-space. 
Additionally, profanity and other naughty words will be removed in order to 
prevent the prediction of such words. While these may be perfectly valid 
predictions, it would be unaccepable to suggest these words while a user is
typing. Since the full data set is so enormous (241+ million words), I will also
downsample the data (randomly) to include only 10% of the full data set. 

After down-sampling the dataset, we are left with the following total number
of words:
```{r subset info, cache = TRUE, echo = FALSE}
subsetcorpus <- corpus(subSet2)
ntoken(subsetcorpus)
```

```{r conditioning, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
library("RWeka")
library("tm")

profile <- "profanewords.txt"
profcon <- file(profile, open="r")
profanewords <- readLines(profcon)
close(profcon)
rm(profcon)
rm(profile)

# create the corpus from the samples
SubCorp <- VCorpus(VectorSource(subSet2))
SubCorp <- tm_map(SubCorp, PlainTextDocument)
SubCorp <- tm_map(SubCorp, content_transformer(tolower))
SubCorp <- tm_map(SubCorp, removeNumbers) 
SubCorp <- tm_map(SubCorp, removePunctuation)
SubCorp <- tm_map(SubCorp, stripWhitespace)
SubCorp <- tm_map(SubCorp, removeWords, profanewords)
```

After performing the cleanup operations, we are left with the following number 
of words. 

```{r post proc analysis, cache = TRUE, echo = FALSE}
thissubcorp <- corpus(SubCorp)
ntoken(thissubcorp)
```

# Analysis

Once the data has beem subsetted and appropriately cleaned, it is now tokenized
(converted into single words). From these tokens, n-grams (1-gram, 2-gram, etc)
and their frequencies are computed. For this exploration, n-grams are computed
for n = 1, 2, and 3 (1-word, 2-word, and 3-word phrases).

```{r ngrams, cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE}
options(java.parameters = "Xmx4g")
library("RWeka")
library("tm")

UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
 
t1 <- TermDocumentMatrix(SubCorp, control = list(tokenize = UnigramTokenizer))
t2 <- TermDocumentMatrix(SubCorp, control = list(tokenize = BigramTokenizer))
t3 <- TermDocumentMatrix(SubCorp, control = list(tokenize = TrigramTokenizer))
```


```{r postprocess, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
t1_mat <- as.matrix(t1)
t2_mat <- as.matrix(t2)
t3_mat <- as.matrix(t3)

t1_sort <- sort(t1_mat[,1], decreasing = TRUE)
t2_sort <- sort(t2_mat[,1], decreasing = TRUE)
t3_sort <- sort(t3_mat[,1], decreasing = TRUE)

t1_top <- t1_sort[1:10]
t2_top <- t2_sort[1:10]
t3_top <- t3_sort[1:10]
```

From these computations, we can see that "the" is the most common single word, 
followed by "and" and "for." When looking at 2-word phrases (2-gram or bigram), 
we see that "of the" and "in the" are the most commmonly used. For 3-word phrases
(3-gram or trigram) "one of the" is the most commmon occurence. 

Following are histograms for all the 1-grams, 2-grams, and 3-grams. These plots
show the word/phrase frequencies for the sampled dataset. Plots show only the 
10 most common word/phrases in each group (for ease of display), however the 
full frequencies have been computed using the *TermDOcumentMatrix* function
of the *tm* package.

These frequencies are computed using 1% of the data, however, during exploration,
the same results were achieved using 0.1% of the data. Based on this observation,
it is likely that there is little benefit to using the full dataset and that
processing efficiency can be achieved by appropriate down-sampling of the data.

```{r plots, echo = FALSE}
barplot(t1_top, main = "Word Frequency", col = "blue")
barplot(t2_top, main = "2-gram Frequency", col = "blue")
barplot(t3_top, main = "3-gram Frequency", col = "blue")
```

# Conclusions

Based on the example above, if a user types the word "of" or "one of," the 
system should probably suggest to the user that the next word would be "the." 
From here, I will investigate several machine learning and prediction approaches 
appropriate for natural language processing and text input prediction. Once an
appropriate approach has been reached, I will create an algorithm that will
predict three possible words as the next word when given up to 5 prior words. 
The suggested words will be colored according to the probability or confidence
in their prediction.

